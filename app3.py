import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import json
import os
from urllib.parse import urljoin

# OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã«å‚™ãˆã¦APIã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦æ‰±ã†
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
    OPENAI_AVAILABLE = False

base_url = "https://rara.jp/hotaruika-toyama/"
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# APIã‚­ãƒ¼è¨­å®š - ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ã‹ã€ç›´æ¥è¨­å®š
api_key = os.environ.get("GROK_API_KEY", "YOUR_GROK_API_KEY")

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
if OPENAI_AVAILABLE:
    try:
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.x.ai/v1"
        )
    except Exception as e:
        print(f"OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        OPENAI_AVAILABLE = False

# APIãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†é¡é–¢æ•°
def simple_classify_comment(text):
    """APIãŒä½¿ãˆãªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡æ©Ÿèƒ½"""
    text = text.lower()
    
    # ãªã—: ãƒ›ã‚¿ãƒ«ã‚¤ã‚«ãŒå…¨ãè¦‹ã‚‰ã‚Œãªã‹ã£ãŸ
    if any(word in text for word in ["ã‚¼ãƒ­", "0åŒ¹", "0æ¯", "ãªã—", "ã„ãªã„", "è¦‹ãˆãªã„"]):
        return "ãªã—"
    
    # æ•°å€¤ãŒç›´æ¥å«ã¾ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹
    number_matches = re.findall(r'(\d+)(?:åŒ¹|æ¯)', text)
    if number_matches:
        count = int(number_matches[0])
        if count == 0:
            return "ãªã—"
        elif count <= 10:
            return "å°‘ãªã„"
        elif count <= 30:
            return "æ™®é€š"
        elif count <= 70:
            return "å¤šã„"
        else:  # 71åŒ¹ä»¥ä¸Š
            return "éå¸¸ã«å¤šã„"
    
    # è¡¨ç¾ã«ã‚ˆã‚‹åˆ†é¡
    if any(word in text for word in ["ãŸãã•ã‚“", "ã„ã£ã±ã„", "å¤šã„"]):
        return "å¤šã„"
    elif any(word in text for word in ["ã¡ã‚‰ã»ã‚‰", "å°‘ã—", "å°‘ãªã„"]):
        return "å°‘ãªã„"
    elif any(word in text for word in ["ãã“ãã“", "ã¾ã‚ã¾ã‚"]):
        return "æ™®é€š"
    elif any(word in text for word in ["å¤§é‡", "çˆ†", "ã™ã”ã„"]):
        return "éå¸¸ã«å¤šã„"
    
    # è©²å½“ãªã—ã®å ´åˆ
    return "ä¸æ˜"

def classify_comment_with_grok(text):
    """Grok APIã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†é¡"""
    print(f"\nã‚³ãƒ¡ãƒ³ãƒˆ: {text[:50]}...")
    
    # APIãŒä½¿ãˆãªã„å ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«åˆ†é¡ã‚’ä½¿ç”¨
    if not OPENAI_AVAILABLE:
        result = simple_classify_comment(text)
        print(f"ã‚·ãƒ³ãƒ—ãƒ«åˆ†é¡çµæœ: {result}")
        return result
    
    prompt = """
ã‚ãªãŸã¯ãƒ›ã‚¿ãƒ«ã‚¤ã‚«ã®æ¹§ãé‡ã‚’åˆ†é¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿ã€ãƒ›ã‚¿ãƒ«ã‚¤ã‚«ã®æ¹§ãé‡ã‚’ä»¥ä¸‹ã®5æ®µéšã§åˆ†é¡ã—ã¦ãã ã•ã„ï¼š
- ãªã—: ãƒ›ã‚¿ãƒ«ã‚¤ã‚«ãŒå…¨ãè¦‹ã‚‰ã‚Œãªã‹ã£ãŸã€0åŒ¹ã€ã¾ãŸã¯ã€Œã„ãªã„ã€ã€Œã‚¼ãƒ­ã€ã€Œãªã—ã€ãªã©ã®è¡¨ç¾ã€‚
- å°‘ãªã„: 1ã€œ10åŒ¹ç¨‹åº¦ã€ã¾ãŸã¯ã€Œæ•°åŒ¹ã€ã€Œã¡ã‚‰ã»ã‚‰ã€ã€Œå°‘ã—ã ã‘ã€ãªã©ã®è¡¨ç¾ã€‚
- æ™®é€š: 11ã€œ30åŒ¹ç¨‹åº¦ã€ã¾ãŸã¯ã€Œã¾ã‚ã¾ã‚ã€ã€Œãã“ãã“ã€ã€Œä¾‹å¹´ä¸¦ã¿ã€ãªã©ã®è¡¨ç¾ã€‚
- å¤šã„: 31ã€œ70åŒ¹ç¨‹åº¦ã€ã¾ãŸã¯ã€ŒãŸãã•ã‚“ã€ã€Œã„ã£ã±ã„ã€ã€Œå ªèƒ½ã€ãªã©ã®è¡¨ç¾ã€‚
- éå¸¸ã«å¤šã„: 71åŒ¹ä»¥ä¸Šã€ã¾ãŸã¯ã€Œå¤§é‡ã€ã€Œçˆ†å¯„ã‚Šã€ã€Œã‚¤ã‚«ã ã‚‰ã‘ã€ãªã©ã®è¡¨ç¾ã€‚
- ä¸æ˜: æ¹§ãé‡ã«é–¢ã™ã‚‹æƒ…å ±ãŒä¸æ˜ç¢ºã€ã¾ãŸã¯å¤©æ°—ï¼ˆã€Œæ³¢ãŒé«˜ã„ã€ï¼‰ã€å¾…æ©Ÿï¼ˆã€Œæ§˜å­è¦‹ã€ï¼‰ã€ãƒªãƒ³ã‚¯ï¼ˆã€Œyoutubeã€ï¼‰ã€è³ªå•ï¼ˆã€Œã©ã†ã§ã™ã‹ï¼Ÿã€ï¼‰ãªã©ã®ç„¡é–¢ä¿‚ãªã‚³ãƒ¡ãƒ³ãƒˆã€‚

**ãƒ«ãƒ¼ãƒ«**:
- ã‚³ãƒ¡ãƒ³ãƒˆãŒæ™‚é–“çµŒéã‚’å«ã‚€å ´åˆï¼ˆä¾‹: ã€Œæœ€åˆã¯ã‚¼ãƒ­ã ã£ãŸãŒå¾Œã§28åŒ¹ã€ï¼‰ã€æœ€æ–°ã®çŠ¶æ…‹ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
- çµµæ–‡å­—ï¼ˆä¾‹: ğŸ˜­ï¼‰ã‚„å£èªè¡¨ç¾ï¼ˆã€Œã‚¤ã‚«ã‚¼ãƒ­ã€ã€Œã‚ã£ã¡ã‚ƒå¤šã„ã€ï¼‰ã‚’è€ƒæ…®ã—ã€æ–‡è„ˆã‚’æ­£ç¢ºã«è§£é‡ˆã—ã¦ãã ã•ã„ã€‚
- æ•°é‡ï¼ˆä¾‹: ã€Œ50åŒ¹ã€ï¼‰ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’æœ€å„ªå…ˆã§åˆ†é¡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- ç„¡é–¢ä¿‚ãªã‚³ãƒ¡ãƒ³ãƒˆã¯å¿…ãšã€Œä¸æ˜ã€ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
- ä¸æ˜ã¯æœ€å¾Œã®é¸æŠè‚¢ã¨ã—ã€å¯èƒ½ãªé™ã‚Šä»–ã®ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚

**å…·ä½“ä¾‹**:
- ã€Œ10åˆ†ã§50åŒ¹ãã‚‰ã„ã®ãƒšãƒ¼ã‚¹ã§ã™ã­ã€â†’ å¤šã„ï¼ˆ50åŒ¹ã¯31ã€œ70åŒ¹ã®ç¯„å›²ï¼‰
- ã€Œ0æ™‚ã‹ã‚‰2æ™‚ã¾ã§ã‚¼ãƒ­ã§ã—ãŸãŒâ€¦28åŒ¹ã§ã—ãŸã€â†’ æ™®é€šï¼ˆæœ€æ–°ã®28åŒ¹ã‚’å„ªå…ˆã€11ã€œ30åŒ¹ï¼‰
- ã€Œäººã‚‚ğŸ¦‘ã‚‚å°‘ãªã—ğŸ˜­ã€â†’ ãªã—ï¼ˆã€Œå°‘ãªã—ã€ã¯ã€Œã„ãªã„ã€ã‚’æ„å‘³ï¼‰
- ã€Œæ³¢å°‘ã—é«˜ã„æ°—é…ãªã—æº€æ½®ã®3æ™‚é ƒã§ã—ã‚‡ã€â†’ ä¸æ˜ï¼ˆå¤©æ°—æƒ…å ±ã¨è³ªå•ï¼‰
- ã€Œ308æ¯ã§ã—ãŸã€â†’ éå¸¸ã«å¤šã„ï¼ˆ71åŒ¹ä»¥ä¸Šï¼‰
- ã€Œãƒãƒ„ãƒãƒ„1æ™‚é–“ã€é›†é­šãƒ©ã‚¤ãƒˆã§é›†ã‚ã¦40åŒ¹ç¾åœ¨ã¯ã€æ­¢ã¾ã‚Šã¾ã—ãŸã€‚ã€â†’ å¤šã„ï¼ˆ40åŒ¹ã¯31ã€œ70åŒ¹ï¼‰
- ã€Œ1:15 ã‚¼ãƒ­ æ¡ã‚Œã‚‹æ—¥ã¯ã“ã®æ™‚é–“å¸¯ã§ã‚‚æ¡ã‚Œã¾ã™ã‹ï¼Ÿã€â†’ ãªã—ï¼ˆã‚¼ãƒ­ã¨è³ªå•ï¼‰
- ã€Œ100åŒ¹ã¾ã§ç²˜ã£ã¦çµ‚äº†ã§ã™ã€â†’ éå¸¸ã«å¤šã„ï¼ˆ71åŒ¹ä»¥ä¸Šï¼‰
- ã€Œæ¿ã‚Šã¯ã»ã‚“ã®å°‘ã—ãƒã‚·ã«ãªã£ãŸç¨‹åº¦ã€ã¾ã ã¾ã æ¿ã£ã¦ã¾ã™ã€â†’ ä¸æ˜ï¼ˆå¤©æ°—æƒ…å ±ï¼‰

**ã‚³ãƒ¡ãƒ³ãƒˆ**: {text}

**åˆ†é¡**: ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
```json
{{
    "surge_level": "ãªã—|å°‘ãªã„|æ™®é€š|å¤šã„|éå¸¸ã«å¤šã„|ä¸æ˜",
    "reason": "åˆ†é¡ã®ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜"
}}
"""
    try:
        response = client.chat.completions.create(
            model="grok-3",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­£ç¢ºãªåˆ†é¡ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt.format(text=text)}
            ],
            max_tokens=200,
            temperature=0.3
        )
        result = json.loads(response.choices[0].message.content)
        surge_level = result["surge_level"]
        reason = result["reason"]
        print(f"åˆ†é¡: {surge_level}ï¼ˆç†ç”±: {reason}ï¼‰")
        return surge_level
    except Exception as e:
        print(f"APIã‚¨ãƒ©ãƒ¼: {e}")
        print("APIã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã‚·ãƒ³ãƒ—ãƒ«åˆ†é¡ã‚’ä½¿ç”¨ã—ã¾ã™")
        result = simple_classify_comment(text)
        print(f"ã‚·ãƒ³ãƒ—ãƒ«åˆ†é¡çµæœ: {result}")
        return result

def get_pagination_urls(base_url, max_pages=5):
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³URLã‚’å–å¾—ã™ã‚‹"""
    urls = [base_url]
    
    try:
        print(f"ãƒ™ãƒ¼ã‚¹URL({base_url})ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        response = requests.get(base_url, headers=headers, timeout=10)
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        
        if response.status_code != 200:
            print(f"ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {response.status_code}")
            return urls
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æä¾›ã•ã‚ŒãŸHTMLæ§‹é€ ã«åŸºã¥ã„ã¦ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’æ¢ã™
        pagination_div = soup.find("div", style="font-size:13px;margin-bottom:4px;")
        
        if pagination_div:
            print("ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            # ã‚¯ãƒ©ã‚¹ 'n' ã‚’æŒã¤ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            page_links = pagination_div.find_all("a", class_="n")
            
            if page_links:
                print(f"æ¤œå‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯æ•°: {len(page_links)}")
                
                for link in page_links:
                    # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆãŒãƒšãƒ¼ã‚¸ç•ªå·ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ>>, << ãªã©ã‚’é™¤å¤–ï¼‰
                    link_text = link.text.strip()
                    if link_text.isdigit() and link_text != "1":  # 1ãƒšãƒ¼ã‚¸ç›®ã¯æ—¢ã«ãƒªã‚¹ãƒˆã«ã‚ã‚‹
                        # hrefå±æ€§ã‚’å–å¾—ï¼ˆ'link2', 'link3'ãªã©ï¼‰
                        href = link.get('href')
                        
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        absolute_url = urljoin(base_url, href)
                        
                        # æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ã‚’è¶…ãˆãªã‘ã‚Œã°ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        page_num = int(link_text)
                        if page_num <= max_pages and absolute_url not in urls:
                            print(f"ãƒšãƒ¼ã‚¸{page_num}ã®URLã‚’è¿½åŠ : {absolute_url}")
                            urls.append(absolute_url)
                            
                            if len(urls) >= max_pages:
                                break
            else:
                print("ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            print("ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    except requests.RequestException as e:
        print(f"ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
    
    # URLã‚’æ¤œè¨¼
    validated_urls = []
    for url in urls:
        try:
            print(f"URLã‚’æ¤œè¨¼ä¸­: {url}")
            test_response = requests.head(url, headers=headers, timeout=5)
            if test_response.status_code == 200:
                print(f"æœ‰åŠ¹ãªURL: {url}")
                validated_urls.append(url)
            else:
                print(f"ç„¡åŠ¹ãªURL ({test_response.status_code}): {url}")
        except requests.RequestException as e:
            print(f"URLæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {url} - {e}")
    
    print(f"å‡¦ç†ã™ã‚‹æœ‰åŠ¹ãªURLä¸€è¦§: {validated_urls}")
    return validated_urls[:max_pages]

def main():
    print("ãƒ›ã‚¿ãƒ«ã‚¤ã‚«æ¹§ãé‡ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    
    # ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¡¨ç¤º
    print(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒšãƒ¼ã‚¸åˆ†ï¼‰
    all_data = []
    
    # æœ€å¤§5ãƒšãƒ¼ã‚¸ã¾ã§å‡¦ç†
    urls_to_scrape = get_pagination_urls(base_url, max_pages=5)
    
    for i, url in enumerate(urls_to_scrape):
        print(f"\n[{i+1}/{len(urls_to_scrape)}] URLã‚’å‡¦ç†ä¸­: {url}")
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            
            if response.status_code != 200:
                print(f"ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {response.status_code}")
                continue
                
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            comments = soup.find_all("table", class_="layer")
            print(f"å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆæ•°: {len(comments)}")

            if not comments:
                print("ã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ã‚’ç¢ºèªã—ã¾ã™...")
                # HTMLã®é ­éƒ¨ã‚’è¡¨ç¤ºã—ã¦æ§‹é€ ç¢ºèª
                html_preview = soup.prettify()[:1000]
                print(f"HTML preview: {html_preview}")
                continue

            for comment_idx, comment in enumerate(comments):
                try:
                    print(f"\nã‚³ãƒ¡ãƒ³ãƒˆ {comment_idx+1}/{len(comments)} å‡¦ç†ä¸­...")
                    
                    # æ—¥ä»˜ã®å–å¾—
                    date_elem = comment.find("div", style=lambda s: s and "float:left" in s)
                    
                    if date_elem:
                        date_text = date_elem.text
                        date_match = re.search(r"æŠ•ç¨¿æ—¥: (\d{4}å¹´\d{2}æœˆ\d{2}æ—¥ \d{2}:\d{2})", date_text)
                        date = date_match.group(1) if date_match else "ä¸æ˜"
                    else:
                        date = "ä¸æ˜"

                    # ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
                    text_elem = comment.find("td", style=lambda s: s and "font-size:15px" in s)
                    if text_elem and text_elem.find("span"):
                        text = text_elem.find("span").text.strip()
                    else:
                        text = ""
                    
                    if date != "ä¸æ˜" and text:
                        print(f"æ—¥ä»˜: {date}")
                        predicted_label = classify_comment_with_grok(text)
                        entry = {
                            "date": date,
                            "comment": text,
                            "surge_level": predicted_label,
                            "source_url": url,
                            "page_number": i+1
                        }
                        all_data.append(entry)
                        print(f"ãƒ‡ãƒ¼ã‚¿è¿½åŠ : {date}, {predicted_label}")
                    else:
                        print(f"ã‚¹ã‚­ãƒƒãƒ—: æ—¥ä»˜={date}, ã‚³ãƒ¡ãƒ³ãƒˆé•·={len(text)}æ–‡å­—")
                except Exception as e:
                    print(f"ã‚³ãƒ¡ãƒ³ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            # ç¾åœ¨ã¾ã§ã®é€²æ—çŠ¶æ³ã‚’è¡¨ç¤º
            print(f"\nç¾åœ¨ã®ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_data)}")
            
            # ä¸€æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä¿å­˜ï¼ˆãƒšãƒ¼ã‚¸ã”ã¨ã®å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã—ãªã„ï¼‰
            if all_data:
                backup_df = pd.DataFrame(all_data)
                backup_filename = "hotaruika_backup.csv"
                backup_df.to_csv(backup_filename, index=False, encoding="utf-8-sig")
                print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {backup_filename} (ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(all_data)})")
            
        except requests.RequestException as e:
            print(f"ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            print(f"ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            
        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
        if i < len(urls_to_scrape) - 1:
            wait_time = 3
            print(f"{wait_time}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(wait_time)

    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
    df = pd.DataFrame(all_data)
    print(f"\næœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
    
    if len(df) == 0:
        print("è­¦å‘Š: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ã‹ã€ã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    elif len(df) <= 50:
        print("è­¦å‘Š: ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã§ã™ï¼ˆ50ä»¶ä»¥ä¸‹ï¼‰ã€‚è¿½åŠ ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ã‹ã€å‡¦ç†ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    # æœ€çµ‚CSVã‚’ä¿å­˜
    final_filename = "hotaruika_surge_data.csv"
    df.to_csv(final_filename, index=False, encoding="utf-8-sig")
    print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {final_filename} (ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)})")

if __name__ == "__main__":
    main()